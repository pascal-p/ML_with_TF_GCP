{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N22NDjIjjxcI"
   },
   "source": [
    "# Distributed Training with GPUs on Cloud AI Platform\n",
    "\n",
    "**Learning Objectives:**\n",
    "  1. Setting up the environment\n",
    "  1. Create a model to train locally\n",
    "  1. Train on multiple GPUs/CPUs with MultiWorkerMirrored Strategy\n",
    "\n",
    "In this notebook, we will walk through using Cloud AI Platform to perform distributed training using the `MirroredStrategy` found within `tf.keras`. This strategy will allow us to use the synchronous AllReduce strategy on a VM with multiple GPUs attached.\n",
    "\n",
    "Each learning objective will correspond to a __#TODO__ in this student lab notebook -- try to complete this notebook first and then review the [Solution Notebook](https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/machine_learning/deepdive2/production_ml/solutions/distributed_training.ipynb) for reference. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nny3m465gKkY"
   },
   "outputs": [],
   "source": [
    "!sudo chown -R jupyter:jupyter /home/jupyter/training-data-analyst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "63e-3SdLlh7K"
   },
   "source": [
    "Next we will configure our environment. Be sure to change the `PROJECT_ID` variable in the below cell to your Project ID. This will be the project to which the Cloud AI Platform resources will be billed. We will also create a bucket for our training artifacts (if it does not already exist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task #1: Setting up the environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "id": "TKZYJbBkBcOk",
    "outputId": "8dac5564-c864-4db8-9cc2-0c8036b75eb8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# TODO 1\n",
    "PROJECT_ID = \"qwiklabs-gcp-03-3fcd4f8a7fba\"  # Replace with your PROJECT\n",
    "BUCKET = PROJECT_ID \n",
    "REGION = 'australia-southeast1'\n",
    "os.environ[\"PROJECT_ID\"] = PROJECT_ID\n",
    "os.environ[\"BUCKET\"] = BUCKET\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jth9W8NtmNUD"
   },
   "source": [
    "Since we are going to submit our training job to Cloud AI Platform, we need to create our trainer package. We will create the `train` directory for our package and create a blank `__init__.py` file so Python knows that this folder contains a package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "cavM78bf_mU4"
   },
   "outputs": [],
   "source": [
    "!mkdir train\n",
    "!touch train/__init__.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PIMQo_lImhn_"
   },
   "source": [
    "Next we will create a module containing a function which will create our model. Note that we will be using the Fashion MNIST dataset. Since it's a small dataset, we will simply load it into memory for getting the parameters for our model.\n",
    "\n",
    "Our model will be a DNN with only dense layers, applying dropout to each hidden layer. We will also use ReLU activation for all hidden layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "88-9WeCQ_mU9",
    "outputId": "ae92afd1-93bd-49e5-aeda-6f6e177ac186"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/model_definition.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/model_definition.py\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# Get data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# add empty color dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "def create_model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Flatten(input_shape=x_train.shape[1:]))\n",
    "    model.add(tf.keras.layers.Dense(1028))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(512))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(256))\n",
    "    model.add(tf.keras.layers.Activation('relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.5))\n",
    "    model.add(tf.keras.layers.Dense(10))\n",
    "    model.add(tf.keras.layers.Activation('softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0DOKsnDhnU87"
   },
   "source": [
    "Before we submit our training jobs to Cloud AI Platform, let's be sure our model runs locally. We will call the `model_definition` function to create our model and use `tf.keras.datasets.fashion_mnist.load_data()` to import the Fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task #2: Create a model to train locally\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from train import model_definition\n",
    "\n",
    "def create_dataset(X, Y, epochs, batch_size):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "    return dataset.repeat(epochs).batch(batch_size, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "id": "r8bPcX7T-SH1",
    "outputId": "1069992b-0599-4b19-f7e8-b4cefa7cb6bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "240/240 [==============================] - 134s 558ms/step - loss: 5.0578 - sparse_categorical_accuracy: 0.6339 - val_loss: 0.6118 - val_sparse_categorical_accuracy: 0.8129\n",
      "Training time without GPUs locally: 134.2877902984619\n"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# add empty color dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "ds_train = create_dataset(x_train, y_train, 20, 5000)\n",
    "ds_test = create_dataset(x_test, y_test, 1, 1000)\n",
    "model = model_definition.create_model()\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=['sparse_categorical_accuracy']\n",
    ")\n",
    "    \n",
    "start = time.time()\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_test, \n",
    "    verbose=1\n",
    ")\n",
    "print(f\"Training time without GPUs locally: {time.time() - start} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L_U-u_tZ_mVK"
   },
   "source": [
    "\n",
    "\n",
    "## Train on multiple GPUs/CPUs with MultiWorkerMirrored Strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P0VQ7GUsn8wb"
   },
   "source": [
    "That took a few minutes to train our model for 20 epochs. Let's see how we can do better using Cloud AI Platform. We will be leveraging the `MultiWorkerMirroredStrategy` supplied in `tf.distribute`. The main difference between this code and the code from the local test is that we need to compile the model within the scope of the strategy. When we do this our training op will use information stored in the `TF_CONFIG` variable to assign ops to the various devices for the AllReduce strategy. \n",
    "\n",
    "After the training process finishes, we will print out the time spent training. Since it takes a few minutes to spin up the resources being used for training on Cloud AI Platform, and this time can vary, we want a consistent measure of how long training took.\n",
    "\n",
    "Note: When we train models on Cloud AI Platform, the `TF_CONFIG` variable is automatically set. So we do not need to worry about adjusting based on what cluster configuration we use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_AF4VDhT_mVg",
    "outputId": "e2e1a496-a369-4f62-856a-b2fe88178add"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing train/train_mult_worker_mirrored.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile train/train_mult_worker_mirrored.py\n",
    "\n",
    "strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()\n",
    "\n",
    "# Get data\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# add empty color dimension\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "ds_train = create_dataset(x_train, y_train, 20, 5000)\n",
    "ds_test = create_dataset(x_test, y_test, 1, 1000)\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "with strategy.scope():\n",
    "    model = model_definition.create_model()\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['sparse_categorical_accuracy']\n",
    "    )\n",
    "    \n",
    "start = time.time()\n",
    "model.fit(\n",
    "    ds_train,\n",
    "    validation_data=ds_test, \n",
    "    verbose=2\n",
    ")\n",
    "print(f\"Training time with multiple GPUs: {time.time() - start} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lab Task #3: Training with multiple GPUs/CPUs on created model using MultiWorkerMirrored Strategy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ViUZcz7Tp9Kp"
   },
   "source": [
    "First we will train a model without using GPUs to give us a baseline. We will use a consistent format throughout the trials. We will define a `config.yaml` file to contain our cluster configuration and the pass this file in as the value of a command-line argument `--config`.\n",
    "\n",
    "In our first example, we will use a single `n1-highcpu-16` VM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sBJw5hJAadVW",
    "outputId": "8f377e27-e1c3-4a44-e5ef-21c37a7f64fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# TODO 3a\n",
    "# Configure a master worker\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-highcpu-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "_mlylgvCaeXW",
    "outputId": "383eb016-e791-4cfc-f382-72acd47932b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: cpu_only_fashion_minst_20220307_091331\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [cpu_only_fashion_minst_20220307_091331] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe cpu_only_fashion_minst_20220307_091331\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs cpu_only_fashion_minst_20220307_091331\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "now=$(date +\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=\"cpu_only_fashion_minst_$now\"\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --package-path=train \\\n",
    "  --module-name=train.train_mult_worker_mirrored \\\n",
    "  --runtime-version=2.3 \\\n",
    "  --python-version=3.7 \\\n",
    "  --region=australia-southeast1 \\\n",
    "  --config config.yaml\n",
    "\n",
    "# us-west1, rather than australia-southeast1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g9tji3XRqbi-"
   },
   "source": [
    "If we go through the logs, we see that the training job will take around 5-7 minutes to complete. Let's now attach two Nvidia Tesla K80 GPUs and rerun the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "fCGWARBH_mVi",
    "outputId": "64f42735-7356-40c4-8460-afc096a9896c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# TODO 3b\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-highcpu-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 2\n",
    "      type: NVIDIA_TESLA_K80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "UmXeeg6r_mVk",
    "outputId": "0017abd4-077d-4089-f664-d15dac69f755"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: multi_gpu_fashion_minst_2gpu_20220307_091536\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [multi_gpu_fashion_minst_2gpu_20220307_091536] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe multi_gpu_fashion_minst_2gpu_20220307_091536\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs multi_gpu_fashion_minst_2gpu_20220307_091536\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "now=$(date +\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=\"multi_gpu_fashion_minst_2gpu_$now\"\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --package-path=train \\\n",
    "  --module-name=train.train_mult_worker_mirrored \\\n",
    "  --runtime-version=2.3 \\\n",
    "  --python-version=3.7 \\\n",
    "  --region=us-west1 \\\n",
    "  --config config.yaml\n",
    "\n",
    "# us-west1, rather than australia-southeast1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0fMLxLOgq7mc"
   },
   "source": [
    "That was a lot faster! The training job will take upto 5-10 minutes to complete. Let's keep going and add more GPUs!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "ocXYk61hGRG_",
    "outputId": "ff6d1dc4-ab02-4ab7-acea-2420c5c1d5aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile config.yaml\n",
    "# TODO 3c\n",
    "trainingInput:\n",
    "  scaleTier: CUSTOM\n",
    "  masterType: n1-highcpu-16\n",
    "  masterConfig:\n",
    "    acceleratorConfig:\n",
    "      count: 4\n",
    "      type: NVIDIA_TESLA_K80"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "id": "MKRRVDLhZoj3",
    "outputId": "4012bb48-d7f9-4e6b-d034-daa258cc636f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "jobId: multi_gpu_fashion_minst_4gpu_20220307_091722\n",
      "state: QUEUED\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job [multi_gpu_fashion_minst_4gpu_20220307_091722] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs describe multi_gpu_fashion_minst_4gpu_20220307_091722\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ai-platform jobs stream-logs multi_gpu_fashion_minst_4gpu_20220307_091722\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "now=$(date +\"%Y%m%d_%H%M%S\")\n",
    "JOB_NAME=\"multi_gpu_fashion_minst_4gpu_$now\"\n",
    "\n",
    "gcloud ai-platform jobs submit training $JOB_NAME \\\n",
    "  --staging-bucket=gs://$BUCKET \\\n",
    "  --package-path=train \\\n",
    "  --module-name=train.train_mult_worker_mirrored \\\n",
    "  --runtime-version=2.3 \\\n",
    "  --python-version=3.7 \\\n",
    "  --region=us-west1 \\\n",
    "  --config config.yaml\n",
    "\n",
    "# us-west1, rather than australia-southeast1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F3PfreD5rfgE"
   },
   "source": [
    "The training job will take upto 10 minutes to complete. It was faster than no GPUs, but why was it slower than 2 GPUs? If you rerun this job with 8 GPUs you'll actually see it takes just as long as using no GPUs!\n",
    "\n",
    "The answer is in our input pipeline. In short, the I/O involved in using more GPUs started to outweigh the benefits of having more available devices. We can try to improve our input pipelines to overcome this (e.g. using caching, adjusting batch size, etc.). \n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Multi-GPU Training on CAIP",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m90",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m90"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
