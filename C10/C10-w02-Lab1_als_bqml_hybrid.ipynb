{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hybrid Recommendations with the Movie Lens Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Note:__ It is recommended that you complete the companion [__als_bqml.ipynb__](../labs/als_bqml.ipynb) notebook before continuing with this __als_bqml_hybrid.ipynb__ notebook. If you already have the movielens dataset and trained model you can skip the \"Import the dataset and trained model\" section.\n",
    "\n",
    "## Learning Objectives\n",
    "1. Know how to extract user and product factors from a BigQuery Matrix Factorizarion Model\n",
    "2. Know how to format inputs for a BigQuery Hybrid Recommendation Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "environ({'SHELL': '/bin/bash', 'OS_IMAGE_FAMILY': 'debian-10', 'CONDA_EXE': '/opt/conda/bin/conda', '_CE_M': '', 'DL_ANACONDA_HOME': '/opt/conda', 'FRAMEWORK_FILE_PATH': '/opt/deeplearning/metadata/framework', 'POST_STARTUP_SCRIPT_PATH': '/opt/c2d/post_start.sh', 'DL_PATH_DEPS': '/opt/deeplearning/deps', 'DL_BIN_PATH': '/opt/deeplearning/bin', 'BINARIES_PATH': '/opt/deeplearning/binaries', 'PWD': '/home/jupyter', 'GCE_METADATA_SERVER_IP': '169.254.169.254', 'LOGNAME': 'jupyter', 'CONDA_PREFIX': '/opt/conda', 'TENSORBOARD_PROXY_URL': '/proxy/%PORT%/', 'PACKAGE_SOURCE_CODE_PATH': '/opt/deeplearning/src/third_party', 'JUPYTER_DEPS_PATH': '/opt/deeplearning/jupyter', 'ENV_URI_FILE_PATH': '/opt/deeplearning/metadata/env_uri', 'HOME': '/home/jupyter', 'RESTRICTION_TYPE_FILE_PATH': '/opt/deeplearning/restriction', 'TF_BINARIES_VERSION': '2-6', 'LANG': 'C.UTF-8', 'TF_BINARIES_FOLDER': '/opt/deeplearning/binaries/tensorflow', 'ENV_VERSION_FILE_PATH': '/opt/deeplearning/metadata/env_version', 'CONDA_PROMPT_MODIFIER': '(base) ', 'SPOOF_METADATA_SERVER_SUBNET': '192.168.10.0/24', 'INVOCATION_ID': 'b9af8416b35f4759846871669649e456', 'LIT_PROXY_URL': '/proxy/%PORT%/', 'TITLE_FILE_PATH': '/opt/deeplearning/metadata/title', '_CE_CONDA': '', 'VERSION_FILE_PATH': '/opt/deeplearning/metadata/version', 'USER': 'jupyter', 'CONDA_SHLVL': '1', 'SHLVL': '0', 'SPOOF_METADATA_SERVER_CONTAINER_NAME': 'spoof-metadata-server', 'CONDA_PYTHON_EXE': '/opt/conda/bin/python', 'OS_IMAGE_PROJECT': 'debian-cloud', 'LD_LIBRARY_PATH': '/usr/local/cuda/lib64:/usr/local/nccl2/lib:/usr/local/cuda/extras/CUPTI/lib64', 'OS_UBUNTU2004': 'ubuntu-2004-lts', 'CONDA_DEFAULT_ENV': 'base', 'OPAL_PREFIX': '/usr', 'OS_UBUNTU1804': 'ubuntu-1804-lts', 'JOURNAL_STREAM': '9:15967', 'DL_PATH': '/opt/deeplearning', 'SPOOF_METADATA_SERVER_NETWORK': 'notebooks-network', 'SPOOF_METADATA_SERVER_IP': '192.168.10.5', 'WORKSPACE_PATH': '/opt/deeplearning/workspace', 'SRC_PATH': '/opt/deeplearning/src', 'PATH': '/usr/local/cuda/bin:/opt/conda/bin:/opt/conda/condabin:/usr/local/bin:/usr/bin:/bin:/usr/local/games:/usr/games', 'OS_DEBIAN10': 'debian-10', 'OS_DEBIAN_PROJECT': 'debian-cloud', 'OS_DEBIAN9': 'debian-9', 'TUTORIALS_PATH': '/opt/deeplearning/workspace/tutorials', 'DL_METADATA_PATH': '/opt/deeplearning/metadata', 'SPOOF_METADATA_SERVER_CONTAINER_IMAGE': 'gcr.io/cloud-builders/metadata', '_': '/opt/conda/bin/jupyter', 'GIT_PYTHON_REFRESH': 'quiet', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'JPY_PARENT_PID': '1197', 'TERM': 'xterm-color', 'CLICOLOR': '1', 'PAGER': 'cat', 'GIT_PAGER': 'cat', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'TF2_BEHAVIOR': '1', 'PROJECT': 'qwiklabs-gcp-02-599caf81836d', 'TFVERSION': '2.5'})\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "PROJECT = \"qwiklabs-gcp-02-599caf81836d\" # REPLACE WITH YOUR PROJECT ID\n",
    "\n",
    "# Do not change these\n",
    "os.environ[\"PROJECT\"] = PROJECT\n",
    "os.environ[\"TFVERSION\"] = '2.5'\n",
    "print(os.environ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the dataset and trained model\n",
    "In the previous notebook, you imported 20 million movie recommendations and trained an ALS model with BigQuery ML\n",
    "\n",
    "To save you the steps of having to do so again (if this is a new environment) you can run the below commands to copy over the clean data and trained model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First create the BigQuery dataset and copy over the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BigQuery error in mk operation: Dataset 'qwiklabs-gcp-02-599caf81836d:movielens'\n",
      "already exists.\n"
     ]
    }
   ],
   "source": [
    "!bq mk movielens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, copy over the trained recommendation model. Note that if you're project is in the EU you will need to change the location from US to EU below. Note that as of the time of writing you cannot copy models across regions with `bq cp`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 'cloud-training-demos:movielens.recommender_16' successfully copied to 'qwiklabs-gcp-02-599caf81836d:movielens.recommender_16'\n",
      "Table 'cloud-training-demos:movielens.recommender_hybrid' successfully copied to 'qwiklabs-gcp-02-599caf81836d:movielens.recommender_hybrid'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Waiting on bqjob_r84e727caca2d8a6_0000017fe82c60c8_1 ... (0s) Current status: DONE   \n",
      "Waiting on bqjob_r17115aec294a52df_0000017fe82c6f7a_1 ... (0s) Current status: DONE   \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "bq --location=US cp \\\n",
    "  cloud-training-demos:movielens.recommender_16 \\\n",
    "  movielens.recommender_16\n",
    "\n",
    "bq --location=US cp \\\n",
    "  cloud-training-demos:movielens.recommender_hybrid \\\n",
    "  movielens.recommender_hybrid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, ensure the model still works by invoking predictions for movie recommendations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 3d9c069c-aedd-4f59-8b26-31fd21b5b0dd\n",
      "Query executing: 0.44s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.movies was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 3d9c069c-aedd-4f59-8b26-31fd21b5b0dd\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT * \n",
    "FROM ML.PREDICT(MODEL `movielens.recommender_16`, (\n",
    "  SELECT movieId, title, 903 AS userId\n",
    "  FROM movielens.movies, UNNEST(genres) g\n",
    "  WHERE g = 'Comedy')\n",
    ")\n",
    "ORDER BY predicted_rating DESC\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Incorporating user and movie information \n",
    "The matrix factorization approach does not use any information about users or movies beyond what is available from the ratings matrix. However, we will often have user information (such as the city they live, their annual income, their annual expenditure, etc.) and we will almost always have more information about the products in our catalog. How do we incorporate this information in our recommendation model?\n",
    "\n",
    "The answer lies in recognizing that the user factors and product factors that result from the matrix factorization approach end up being a concise representation of the information about users and products available from the ratings matrix. We can concatenate this information with other information we have available and train a regression model to predict the rating.\n",
    "### Obtaining user and product factors\n",
    "We can get the user factors or product factors from ML.WEIGHTS. For example to get the product factors for movieId=96481 and user factors for userId=54192, we would do:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.01s: 100%|██████████| 1/1 [00:00<00:00, 619.36query/s]                          \n",
      "Downloading: 100%|██████████| 2/2 [00:01<00:00,  1.21rows/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>processed_input</th>\n",
       "      <th>feature</th>\n",
       "      <th>f0_</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>movieId</td>\n",
       "      <td>96481</td>\n",
       "      <td>[{\"factor\":16,\"weight\":4.7488055154865094e-16}...</td>\n",
       "      <td>-1.398794</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>userId</td>\n",
       "      <td>54192</td>\n",
       "      <td>[{\"factor\":16,\"weight\":1.451774918706243},{\"fa...</td>\n",
       "      <td>1.839885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  processed_input feature                                                f0_  \\\n",
       "0         movieId   96481  [{\"factor\":16,\"weight\":4.7488055154865094e-16}...   \n",
       "1          userId   54192  [{\"factor\":16,\"weight\":1.451774918706243},{\"fa...   \n",
       "\n",
       "   intercept  \n",
       "0  -1.398794  \n",
       "1   1.839885  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT processed_input, feature, TO_JSON_STRING(factor_weights), intercept\n",
    "FROM ML.WEIGHTS(MODEL movielens.recommender_16)\n",
    "WHERE (processed_input = 'movieId' AND feature = '96481')\n",
    "OR (processed_input = 'userId' AND feature = '54192')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying these weights and adding the intercept is how we get the predicted rating for this combination of `movieId` and `userId` in the matrix factorization approach.\n",
    "\n",
    "These weights also serve as a low-dimensional representation of the movie and user behavior. We can create a regression model to predict the rating given the user factors, product factors, and any other information we know about our users and products.\n",
    "### Creating input features\n",
    "The MovieLens dataset does not have any user information, and has very little information about the movies themselves. To illustrate the concept, therefore, let’s create some synthetic information about users:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: da164294-ce5d-47e6-8c3d-187f52c2e135\n",
      "Query executing: 0.49s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.ratings was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: da164294-ce5d-47e6-8c3d-187f52c2e135\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "CREATE OR REPLACE TABLE movielens.users AS\n",
    "SELECT userId, RAND() * COUNT(rating) AS loyalty, CONCAT(SUBSTR(CAST(userId AS STRING), 0, 2)) AS postcode\n",
    "FROM movielens.ratings\n",
    "GROUP BY userId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input features about users can be obtained by joining the user table with the ML weights and selecting all the user information and the user factors from the weights array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 7d983740-cf15-4491-9c1f-58e254dcf4b7\n",
      "Query executing: 0.50s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.users was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 7d983740-cf15-4491-9c1f-58e254dcf4b7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "WITH userFeatures AS (\n",
    "  SELECT u.*, (SELECT ARRAY_AGG(weight) FROM UNNEST(factor_weights)) AS user_factors\n",
    "  FROM movielens.users u\n",
    "  JOIN ML.WEIGHTS(MODEL movielens.recommender_16)\n",
    "  ON processed_input = 'userId' AND feature = CAST(u.userId AS STRING)\n",
    ")\n",
    "\n",
    "SELECT * FROM userFeatures\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly, we can get product features for the movies data, except that we have to decide how to handle the genre since a movie could have more than one genre. If we decide to create a separate training row for each genre, then we can construct the product features using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 55ea4da8-83c9-4e7d-8167-8954cc1f7512\n",
      "Query executing: 0.55s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.movies was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 55ea4da8-83c9-4e7d-8167-8954cc1f7512\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "WITH productFeatures AS (\n",
    "  SELECT p.* EXCEPT(genres), g, \n",
    "        (SELECT ARRAY_AGG(weight) FROM UNNEST(factor_weights)) AS product_factors\n",
    "  FROM movielens.movies p, UNNEST(genres) g\n",
    "  JOIN ML.WEIGHTS(MODEL movielens.recommender_16) \n",
    "  ON processed_input = 'movieId' AND feature = CAST(p.movieId AS STRING)\n",
    ")\n",
    "\n",
    "SELECT * FROM productFeatures\n",
    "LIMIT 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining these two WITH clauses and pulling in the rating corresponding the movieId-userId combination (if it exists in the ratings table), we can create the training dataset.\n",
    "\n",
    "**TODO 1**: Combine the above two queries to get the user factors and product factor for each rating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The below cell will take approximately 4~5 minutes for the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 4c037708-0d88-4728-9c2c-5f2f0fdf4c65\n",
      "Query executing: 0.46s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.movies was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 4c037708-0d88-4728-9c2c-5f2f0fdf4c65\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "CREATE OR REPLACE TABLE movielens.hybrid_dataset AS\n",
    "\n",
    "    WITH userFeatures AS (\n",
    "        # TODO: Place the user features query here\n",
    "        SELECT u.*, (SELECT ARRAY_AGG(weight) FROM UNNEST(factor_weights)) AS user_factors\n",
    "        FROM movielens.users u\n",
    "        JOIN ML.WEIGHTS(MODEL movielens.recommender_16)\n",
    "        ON processed_input = 'userId' AND feature = CAST(u.userId AS STRING)\n",
    "    ),\n",
    "\n",
    "    productFeatures AS (\n",
    "        # TODO: Place the product features query here\n",
    "        SELECT p.* EXCEPT(genres), g, \n",
    "            (SELECT ARRAY_AGG(weight) FROM UNNEST(factor_weights)) AS product_factors\n",
    "        FROM movielens.movies p, UNNEST(genres) g\n",
    "        JOIN ML.WEIGHTS(MODEL movielens.recommender_16) \n",
    "        ON processed_input = 'movieId' AND feature = CAST(p.movieId AS STRING)\n",
    "    )\n",
    "\n",
    "    SELECT p.* EXCEPT(movieId), u.* EXCEPT(userId), rating \n",
    "    FROM productFeatures p, userFeatures u\n",
    "    JOIN movielens.ratings r\n",
    "    ON r.movieId = p.movieId AND r.userId = u.userId"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One of the rows of this table looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 919b5697-0c14-4822-a514-9a89dba02dcc\n",
      "Query executing: 0.48s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.hybrid_dataset was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 919b5697-0c14-4822-a514-9a89dba02dcc\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT *\n",
    "FROM movielens.hybrid_dataset\n",
    "LIMIT 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essentially, we have a couple of attributes about the movie, the product factors array corresponding to the movie, a couple of attributes about the user, and the user factors array corresponding to the user. These form the inputs to our “hybrid” recommendations model that builds off the matrix factorization model and adds in metadata about users and movies.\n",
    "### Training hybrid recommendation model\n",
    "At the time of writing, BigQuery ML can not handle arrays as inputs to a regression model. Let’s, therefore, define a function to convert arrays to a struct where the array elements are its fields:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "CREATE OR REPLACE FUNCTION movielens.arr_to_input_16_users(u ARRAY<FLOAT64>)\n",
    "RETURNS \n",
    "    STRUCT<\n",
    "        u1 FLOAT64,\n",
    "        u2 FLOAT64,\n",
    "        u3 FLOAT64,\n",
    "        u4 FLOAT64,\n",
    "        u5 FLOAT64,\n",
    "        u6 FLOAT64,\n",
    "        u7 FLOAT64,\n",
    "        u8 FLOAT64,\n",
    "        u9 FLOAT64,\n",
    "        u10 FLOAT64,\n",
    "        u11 FLOAT64,\n",
    "        u12 FLOAT64,\n",
    "        u13 FLOAT64,\n",
    "        u14 FLOAT64,\n",
    "        u15 FLOAT64,\n",
    "        u16 FLOAT64\n",
    "    > AS (STRUCT(\n",
    "        u[OFFSET(0)],\n",
    "        u[OFFSET(1)],\n",
    "        u[OFFSET(2)],\n",
    "        u[OFFSET(3)],\n",
    "        u[OFFSET(4)],\n",
    "        u[OFFSET(5)],\n",
    "        u[OFFSET(6)],\n",
    "        u[OFFSET(7)],\n",
    "        u[OFFSET(8)],\n",
    "        u[OFFSET(9)],\n",
    "        u[OFFSET(10)],\n",
    "        u[OFFSET(11)],\n",
    "        u[OFFSET(12)],\n",
    "        u[OFFSET(13)],\n",
    "        u[OFFSET(14)],\n",
    "        u[OFFSET(15)]\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which gives:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: edc44640-2b8d-42d2-945f-ab075a065d34\n",
      "Query executing: 0.43s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 400 Function not found: movielens.arr_to_input_16_users at [1:8]\n",
      "\n",
      "Location: US\n",
      "Job ID: edc44640-2b8d-42d2-945f-ab075a065d34\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "SELECT movielens.arr_to_input_16_users(u).*\n",
    "FROM (SELECT [0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15.] AS u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create a similar function named movielens.arr_to_input_16_products to convert the product factor array into named columns.\n",
    "\n",
    "**TODO 2**: Create a function that returns named columns from a size 16 product factor array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Query complete after 0.00s: 100%|██████████| 1/1 [00:00<00:00, 965.98query/s] \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: []\n",
       "Index: []"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "CREATE OR REPLACE FUNCTION movielens.arr_to_input_16_products(p ARRAY<FLOAT64>)\n",
    "RETURNS \n",
    "    STRUCT<\n",
    "        p1 FLOAT64,\n",
    "        p2 FLOAT64,\n",
    "        p3 FLOAT64,\n",
    "        p4 FLOAT64,\n",
    "        p5 FLOAT64,\n",
    "        p6 FLOAT64,\n",
    "        p7 FLOAT64,\n",
    "        p8 FLOAT64,\n",
    "        p9 FLOAT64,\n",
    "        p10 FLOAT64,\n",
    "        p11 FLOAT64,\n",
    "        p12 FLOAT64,\n",
    "        # TODO: Finish building this struct\n",
    "        p13 FLOAT64,\n",
    "        p14 FLOAT64,\n",
    "        p15 FLOAT64,\n",
    "        p16 FLOAT64\n",
    "    > AS (STRUCT(\n",
    "        p[OFFSET(0)],\n",
    "        p[OFFSET(1)],\n",
    "        p[OFFSET(2)],\n",
    "        p[OFFSET(3)],\n",
    "        p[OFFSET(4)],\n",
    "        p[OFFSET(5)],\n",
    "        p[OFFSET(6)],\n",
    "        p[OFFSET(7)],\n",
    "        p[OFFSET(8)],\n",
    "        p[OFFSET(9)],\n",
    "        p[OFFSET(10)],\n",
    "        p[OFFSET(11)],\n",
    "        p[OFFSET(12)],\n",
    "        # TODO: Finish building this struct\n",
    "        p[OFFSET(13)],\n",
    "        p[OFFSET(14)],\n",
    "        p[OFFSET(15)]\n",
    "));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, we can tie together metadata about users and products with the user factors and product factors obtained from the matrix factorization approach to create a regression model to predict the rating:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: The below cell will take approximately 25~30 minutes for the completion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing query with job ID: 469f1c36-f8f9-4d55-9dbb-478fd26c748f\n",
      "Query executing: 0.44s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "ERROR:\n",
      " 404 Not found: Table qwiklabs-gcp-02-599caf81836d:movielens.hybrid_dataset was not found in location US\n",
      "\n",
      "Location: US\n",
      "Job ID: 469f1c36-f8f9-4d55-9dbb-478fd26c748f\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%bigquery --project $PROJECT\n",
    "CREATE OR REPLACE MODEL movielens.recommender_hybrid \n",
    "OPTIONS(model_type='linear_reg', input_label_cols=['rating'])\n",
    "AS\n",
    "SELECT * EXCEPT(user_factors, product_factors), movielens.arr_to_input_16_users(user_factors).*,\n",
    "    movielens.arr_to_input_16_products(product_factors).*\n",
    "FROM movielens.hybrid_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no point looking at the evaluation metrics of this model because the user information we used to create the training dataset was fake (not the RAND() in the creation of the loyalty column) -- we did this exercise in order to demonstrate how it could be done. And of course, we could train a dnn_regressor model and optimize the hyperparameters if we want a more sophisticated model. But if we are going to go that far, it might be better to consider using Auto ML tables, covered in the next section.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copyright 2021 Google Inc. Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at http://www.apache.org/licenses/LICENSE-2.0 Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License."
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "tf2-gpu.2-6.m91",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-6:m91"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
